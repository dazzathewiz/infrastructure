---

# 1. Initalise Ceph (Datacentre -> Ceph -> setup Ceph)
# See: https://github.com/lae/ansible-role-proxmox/blob/main/tasks/ceph.yml
- block:
  - name: Create initial Ceph config
    command: "pveceph init --network {{ _ceph_network_frontend_ip }}/{{ pve_ceph_net_front_cidr }}  \
    {% if pve_ceph_network_backend is defined  %} \
                --cluster-network {{ _ceph_network_backend_ip }}/{{ pve_ceph_net_back_cidr }}
    {% endif %}"
    args:
      creates: /etc/ceph/ceph.conf

  - name: Create initial Ceph monitor
    command: 'pveceph mon create'
    args:
      creates: '/var/lib/ceph/mon/ceph-{{ ansible_hostname }}/'
    register: _ceph_initial_mon

  - name: Fail if initial monitor creation failed
    fail:
      msg: 'Ceph intial monitor creation failed.'
    when: _ceph_initial_mon is failed
  when: "inventory_hostname == groups[pve_ceph_mon_group][0]"

# 2. Create additional monitors for other nodes under node -> ceph -> monitor
- name: Create additional Ceph monitors
  command: 'pveceph mon create'
  args:
    creates: '/var/lib/ceph/mon/ceph-{{ ansible_hostname }}/'
  when: 
  - "inventory_hostname != groups[pve_ceph_mon_group][0]"
  - "inventory_hostname in groups[pve_ceph_mon_group]"

# 3. Install the Ceph Manager Dashboard on node 1
- import_tasks: pve_ceph_mgr_dashboard.yml
  when: "inventory_hostname == groups[pve_ceph_mon_group][0]"

# 4. Add redundant manager nodes on remaining nodes
- name: Add ceph-mgr on remaining nodes
  command: 'pveceph mgr create'
  args:
    creates: '/var/lib/ceph/mgr/ceph-{{ ansible_hostname }}/'
  when: "inventory_hostname in groups[pve_ceph_mgr_group]"

# (Optional) Check for existing OSD's on host
# - block:
#   - name: Query for existing Ceph volumes
#     pve_ceph_volume:
#     check_mode: no
#     register: _ceph_volume_data

#   - name: Generate a list of active OSDs
#     ansible.builtin.set_fact:
#       _existing_ceph_osds: "{{ _ceph_volume_data.stdout | from_json | json_query('*[].devices[]') | default([]) }}"
  
#   - name: debug
#     debug:
#       msg: "{{_existing_ceph_osds}}"

# (Optional) Define expected OSD's and create OSD's if not present
# - block:
#   - name: Generate list of unprovisioned OSDs
#     ansible.builtin.set_fact:
#       _ceph_osds_diff: "{{ _ceph_osds_diff | default([]) + [item] }}"
#     loop: "{{ pve_ceph_osds }}"
#     when: item.device not in _existing_ceph_osds

#   - name: Create Ceph OSDs
#     ansible.builtin.command: >-
#       pveceph osd create {{ item.device }}
#       {% if "encrypted" in item and item["encrypted"] | bool %}--encrypted 1{% endif %}
#       {% if "block.db" in item %}--db_dev {{ item["block.db"] }}{% endif %}
#       {% if "block.wal" in item %}--wal_dev {{ item["block.wal"] }}{% endif %}
#     loop: '{{ _ceph_osds_diff | default([]) }}'


# 8. Create the Pools in the Ceph manager
#   https://10.10.10.18:8443/#/pool
#   Create a replicated pool and an ErasureCoded (EC) pool. Ensure the EC pool has "EC Overwrite" enabled

# 9. Add the ceph replicated pool to proxmox Datacentre -> Storage -> add RBD

# 10. Change the data-pool for the ceph replica pool in /etc/pve/storage.cfg:
#rbd: ceph-vm
#        content images,rootdir
#        krbd 0
#        pool Ceph_Prox_MetaDataREP
#        data-pool Ceph_NVME_EC3

# 11. Creating cephfs; Ref: https://pve.proxmox.com/wiki/Deploy_Hyper-Converged_Ceph_Cluster#_footnote_16
#   Create a _metadata pool with a replica set (required for metadata)
#   Create a _data pool with erasurecode as desired
#   Go to node -> Ceph -> CephFS -> Create Meta Data servers
#   ceph fs new cephfs_plexdata cephfs_plexdata_metadata cephfs_plexdata_data --force
#   ceph fs set cephfs_plexdata max_mds 2
#   (Optional) Can mount the filesystem: Datacentre -> Storage -> Add -> CephFS
# Note: https://pve.proxmox.com/wiki/Storage:_CephFS

# To manage a ceph host reboot
#   To perform maintenance without CRUSH auto rebalancing the cluster:
#     - ceph mds fail {{ host }}
#     - ceph osd set-group noout {{ host }}  # https://docs.ceph.com/en/quincy/rados/troubleshooting/troubleshooting-osd/
#   When complete:
#     - ceph osd unset-group noout {{ host }}

# To Remove/Destroy Ceph pools
#   1. Unmount any of the RBD/CephFS from clients inlucuding proxmox (Datacentre -> Storage -> Remove)
#   2. For CephFS:
#       a) stop (and destroy) all mds under proxmox node -> Ceph -> CephFS
#       b) disable the CephFS filesystem `ceph fs rm cephfs_plexdata --yes-i-really-mean-it`
#   3. Remove Ceph pools from proxmox (cannot be done from Ceph dashboard); Ceph -> Pools -> Destroy
#
# Other undo steps:
#   1. Remove mgr's: `pveceph mgr destroy localhost|prox2|prox3` - run on individual host
#   2. Uninstall "ceph-mgr-dashboard" on all hosts `apt remove ceph-mgr-dashboard`
#   3. Delete Mons under node -> ceph -> monitor
#   4. `pveceph purge`
#   5. `rm /etc/ceph/ceph.conf /etc/pve/ceph.conf`
#   6. Cleanup the OSD disks. Note that ceph holds the OSD information in /var/lib/...etc. This is purged
#       and as a result OSD cannot be "re-imported". They need to be destroyed:
#       a) remove LVM volumes in node -> Disks -> LVM -> disk -> more -> Destroy
