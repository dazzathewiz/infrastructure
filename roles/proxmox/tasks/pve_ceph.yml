---

# 1. Initalise Ceph (Datacentre -> Ceph -> setup Ceph)
# See: https://github.com/lae/ansible-role-proxmox/blob/main/tasks/ceph.yml
- block:
  - name: Create initial Ceph config
    command: "pveceph init --network {{ _ceph_network_frontend_ip }}/{{ pve_ceph_net_front_cidr }}  \
    {% if pve_ceph_network_backend is defined  %} \
                --cluster-network {{ _ceph_network_backend_ip }}/{{ pve_ceph_net_back_cidr }}
    {% endif %}"
    args:
      creates: /etc/ceph/ceph.conf

  - name: Create initial Ceph monitor
    command: 'pveceph mon create'
    args:
      creates: '/var/lib/ceph/mon/ceph-{{ ansible_hostname }}/'
    register: _ceph_initial_mon

  - name: Fail if initial monitor creation failed
    fail:
      msg: 'Ceph intial monitor creation failed.'
    when: _ceph_initial_mon is failed
  when: "inventory_hostname == groups[pve_ceph_mon_group][0]"

# 2. Create additional monitors for other nodes under node -> ceph -> monitor
- name: Create additional Ceph monitors
  command: 'pveceph mon create'
  args:
    creates: '/var/lib/ceph/mon/ceph-{{ ansible_hostname }}/'
  when: 
  - "inventory_hostname != groups[pve_ceph_mon_group][0]"
  - "inventory_hostname in groups[pve_ceph_mon_group]"

# 3. Install the Ceph Manager Dashboard on node 1
- import_tasks: pve_ceph_mgr_dashboard.yml
  when: "inventory_hostname == groups[pve_ceph_mon_group][0]"

# 4. Add redundant manager nodes on remaining nodes
- name: Add ceph-mgr on remaining nodes
  command: 'pveceph mgr create'
  args:
    creates: '/var/lib/ceph/mgr/ceph-{{ ansible_hostname }}/'
  when: "inventory_hostname in groups[pve_ceph_mgr_group]"

# 7. Create OSD's in each node Ceph -> OSD
#   Create 1 for each 16TB disk, enable encrption, no other settings
#   Run for nvme: `ceph-volume lvm batch --osds-per-device 4 --crush-device-class nvme /dev/nvme1n1` (https://forum.proxmox.com/threads/recommended-way-of-creating-multiple-osds-per-nvme-disk.52252/)

# 8. Create the Pools in the Ceph manager
#   https://10.10.10.18:8443/#/pool
#   Create a replicated pool and an ErasureCoded (EC) pool. Ensure the EC pool has "EC Overwrite" enabled

# 9. Add the ceph replicated pool to proxmox Datacentre -> Storage -> add RBD

# 10. Change the data-pool for the ceph replica pool in /etc/pve/storage.cfg:
#rbd: ceph-vm
#        content images,rootdir
#        krbd 0
#        pool Ceph_Prox_MetaDataREP
#        data-pool Ceph_NVME_EC3

# 11. Creating cephfs; Ref: https://pve.proxmox.com/wiki/Deploy_Hyper-Converged_Ceph_Cluster#_footnote_16
#   Create a _metadata pool with a replica set (required for metadata)
#   Create a _data pool with erasurecode as desired
#   Go to node -> Ceph -> CephFS -> Create Meta Data servers
#   ceph fs new cephfs_plexdata cephfs_plexdata_metadata cephfs_plexdata_data --force
#   ceph fs set cephfs_plexdata max_mds 2
#   (Optional) Can mount the filesystem: Datacentre -> Storage -> Add -> CephFS
# Note: https://pve.proxmox.com/wiki/Storage:_CephFS

# To manage a ceph host reboot
#   To perform maintenance without CRUSH auto rebalancing the cluster:
#     - ceph mds fail {{ host }}
#     - ceph osd set-group noout {{ host }}  # https://docs.ceph.com/en/quincy/rados/troubleshooting/troubleshooting-osd/
#   When complete:
#     - ceph osd unset-group noout {{ host }}

# To Remove/Destroy Ceph pools
#   1. Unmount any of the RBD/CephFS from clients inlucuding proxmox (Datacentre -> Storage -> Remove)
#   2. For CephFS:
#       a) stop (and destroy) all mds under proxmox node -> Ceph -> CephFS
#       b) disable the CephFS filesystem `ceph fs rm cephfs_plexdata --yes-i-really-mean-it`
#   3. Remove Ceph pools from proxmox (cannot be done from Ceph dashboard); Ceph -> Pools -> Destroy
#
# Other undo steps:
#   1. Remove mgr's: `pveceph mgr destroy localhost|prox2|prox3` - run on individual host
#   2. Uninstall "ceph-mgr-dashboard" on all hosts `apt remove ceph-mgr-dashboard`
#   3. Delete Mons under node -> ceph -> monitor
#   4. `pveceph purge`
#   5. `rm /etc/ceph/ceph.conf /etc/pve/ceph.conf`
