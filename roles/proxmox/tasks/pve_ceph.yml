---


# 2. Datacentre -> Ceph -> setup Ceph with network 10.10.10.18/28

# 3. Create additional monitors under node -> ceph -> monitor
# 4. Delete the "mon.localhost" monitor created by default on prox1 and re-create it "mon.prox1"

# 5. Install the Ceph Manager Dashboard on Prox1:
#   apt-get install ceph-mgr-dashboard -> This should be installed on all nodes to support failover of the dashboard which needs to run on the active ceph manager instance
#   ceph mgr module enable dashboard
#   ceph dashboard create-self-signed-cert
#   ceph dashboard ac-user-create infadmin -i ceph-password administrator
#   ceph mgr module disable dashboard
#   ceph mgr module enable dashboard

# 6. Add redundant manager nodes on prox2, prox3
#   pveceph mgr create

# 7. Create OSD's in each node Ceph -> OSD
#   Create 1 for each 16TB disk, enable encrption, no other settings
#   Run for nvme: `ceph-volume lvm batch --osds-per-device 4 --crush-device-class nvme /dev/nvme1n1` (https://forum.proxmox.com/threads/recommended-way-of-creating-multiple-osds-per-nvme-disk.52252/)

# 8. Create the Pools in the Ceph manager
#   https://10.10.10.18:8443/#/pool
#   Create a replicated pool and an ErasureCoded (EC) pool. Ensure the EC pool has "EC Overwrite" enabled

# 9. Add the ceph replicated pool to proxmox Datacentre -> Storage -> add RBD

# 10. Change the data-pool for the ceph replica pool in /etc/pve/storage.cfg:
#rbd: ceph-vm
#        content images,rootdir
#        krbd 0
#        pool Ceph_Prox_MetaDataREP
#        data-pool Ceph_NVME_EC3

# 11. Creating cephfs; Ref: https://pve.proxmox.com/wiki/Deploy_Hyper-Converged_Ceph_Cluster#_footnote_16
#   Create a _metadata pool with a replica set (required for metadata)
#   Create a _data pool with erasurecode as desired
#   Go to node -> Ceph -> CephFS -> Create Meta Data servers
#   ceph fs new cephfs_plexdata cephfs_plexdata_metadata cephfs_plexdata_data --force
#   ceph fs set cephfs_plexdata max_mds 2
#   (Optional) Can mount the filesystem: Datacentre -> Storage -> Add -> CephFS
# Note: https://pve.proxmox.com/wiki/Storage:_CephFS

# To manage a ceph host reboot
#   To perform maintenance without CRUSH auto rebalancing the cluster:
#     - ceph mds fail {{ host }}
#     - ceph osd set-group noout {{ host }}  # https://docs.ceph.com/en/quincy/rados/troubleshooting/troubleshooting-osd/
#   When complete:
#     - ceph osd unset-group noout {{ host }}

# To Remove/Destroy Ceph pools
#   1. Unmount any of the RBD/CephFS from clients inlucuding proxmox (Datacentre -> Storage -> Remove)
#   2. For CephFS:
#       a) stop (and destroy) all mds under proxmox node -> Ceph -> CephFS
#       b) disable the CephFS filesystem `ceph fs rm cephfs_plexdata --yes-i-really-mean-it`
#   3. Remove Ceph pools from proxmox (cannot be done from Ceph dashboard); Ceph -> Pools -> Destroy
#
# Other undo steps:
#   1. Remove mgr's: `pveceph mgr destroy localhost|prox2|prox3` - run on individual host
#   2. Uninstall "ceph-mgr-dashboard" on all hosts `apt remove ceph-mgr-dashboard`
#   3. Delete Mons under node -> ceph -> monitor
